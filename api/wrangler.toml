# wrangler.toml — Cognomega API Worker (prod)
# Routes all traffic for api.cognomega.com to a single Worker.

name               = "cognomega-api"
main               = "src/index.ts"
compatibility_date = "2025-09-03"
workers_dev        = false
logpush            = false   # optional (enable in CF dashboard if you want)

# Route everything under the zone to this Worker
routes = [
  { pattern = "api.cognomega.com/*", zone_name = "cognomega.com" }
]

# ---------- Bindings ----------

# Cloudflare Workers AI
[ai]
binding = "AI"

# KV namespaces
# KEYS: JWKS (public JSON at /.well-known/jwks.json)
[[kv_namespaces]]
binding = "KEYS"
id      = "7e9d002852ba44849900a2babfcaf816"

# KV_BILLING: credits + usage + jobs (module-backed)
[[kv_namespaces]]
binding = "KV_BILLING"
id      = "5da314a8b48b4db684678ab5ad074563"

# KV_PREFS: per-user voice preferences (new; infra already created)
[[kv_namespaces]]
binding = "KV_PREFS"
id      = "ef783de622644eb89ff2dacfcc41a2f2"

# R2 buckets
# Keep BOTH: legacy routes may use R2, new module uses R2_UPLOADS.
[[r2_buckets]]
binding     = "R2"                 # legacy artifacts bucket
bucket_name = "cognomega-prod"

[[r2_buckets]]
binding     = "R2_UPLOADS"         # direct-upload bucket (auth/billing module)
bucket_name = "cognomega-uploads"

# ---------- Vars (non-secrets) ----------
[vars]
# CORS allowlist (exact origins; no wildcards).
# Module CORS reads this env var; index.ts also sets CORS and will allow Pages previews.
ALLOWED_ORIGINS = "https://app.cognomega.com,https://www.cognomega.com,https://cognomega.com,https://cognomega-frontend.pages.dev,http://localhost:5173,http://127.0.0.1:5173"
WARN_CREDITS = "5"
USAGE_TTL_DAYS = "30"
JOB_TTL_DAYS   = "14"

# JWT / issuer for guest tokens (RS256 via PRIVATE_KEY_PEM + KID)
ISSUER      = "https://api.cognomega.com"
JWT_TTL_SEC = "3600"
KID         = "k1"

# Billing config
CREDIT_PER_1K     = "0.05"       # credits per 1k tokens
MAX_UPLOAD_BYTES  = "10485760"   # 10MB cap for /api/upload/direct

# Provider order & model defaults (SI/LLM orchestration)
PREFERRED_PROVIDER = "groq,cfai,openai"
GROQ_MODEL   = "llama-3.1-8b-instant"
CF_AI_MODEL  = "@cf/meta/llama-3.1-8b-instruct"
OPENAI_MODEL = "gpt-4o-mini"

# Upstream base URLs (override only if needed)
OPENAI_BASE  = "https://api.openai.com/v1"
# GROQ_BASE  = "https://api.groq.com/openai/v1"  # optional override

# (Optional demo/legacy DB routes used by some index.ts endpoints)
# NEON_DATABASE_URL = "postgres://..."
# DATABASE_URL      = "postgres://..."

# ---------- Secrets (set via `npx wrangler secret put ...`) ----------
# PRIVATE_KEY_PEM     # RS256 PKCS#8 PEM used for signed guest JWTs
# ADMIN_API_KEY       # for /api/credits/adjust and /api/admin/* routes
# GROQ_API_KEY        # if using Groq
# OPENAI_API_KEY      # if using OpenAI
# CARTESIA_API_KEY    # optional (TTS demo)
# ADMIN_TASK_SECRET   # for internal /admin/process-one and cron trigger
# TURNSTILE_SECRET    # (recommended) server-side Turnstile verify for uploads

# ---------- Scheduled tasks ----------
[triggers]
crons = ["*/5 * * * *"]  # queue kicker for Neon jobs (up to 5 per tick)
